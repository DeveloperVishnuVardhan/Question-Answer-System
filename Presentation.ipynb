{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36a6898",
   "metadata": {},
   "source": [
    "## **Task:** Fine-tune a pre-trained Transformer-based chatbot using Tensorflow on the WikiQA corpus dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7368e6",
   "metadata": {},
   "source": [
    "### Approach:\n",
    "1. Exploratory Data Analysis.\n",
    "2. Data cleaning and preparation.\n",
    "3. Model training.\n",
    "4. Testing the model.\n",
    "5. Performing evaluations on Train and Test data.\n",
    "\n",
    "### The pipleline of the system is as follows:\n",
    "* **main.py:** The main file which performs all the tasks from exploratory data analysis, data-preparation, model-training, predictions, evaluations based on command line arguements given by user.\n",
    "* **Data_prep.py:** Contains the code and functions needed to clean all the train_data and transform the data which is ready to be feeded to the model.\n",
    "* **model.py:** This file contains the code to use the transformed data to fine-tune the bert model for a question answering task.\n",
    "* **Predictions.py:** This file contains all the code to perform predictions and evaluations on the testing data and example inputs given by user.\n",
    "* **utils.py:** This file contains all the helper_functions used in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5f47e",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Jyothi Vishnu Vardhan Kolla.\n",
    "\n",
    "This is the main file which performs tasks such as\n",
    "explorataroy data anaysis,data-preparation,model-training based on command line inputs.\n",
    "\"\"\"\n",
    "import sys\n",
    "from Exploratory_data_analysis import ExploratoryAnalysis\n",
    "from Data_prep import PrepareData\n",
    "from models import Models\n",
    "import pandas as pd\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from utils import load_model\n",
    "from Predictions import Predictions, Evaluations\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    \"\"\"\n",
    "        Main which takes command line arguements and executes the pipeline.\n",
    "\n",
    "        ARGS:\n",
    "            argv[1]: if given-1 performs exploratory data analysis on given data.\n",
    "            argv[2]: if given-1 performing preprocessing and prepares the final data.\n",
    "            argv[3]: if given-1 performs the training.\n",
    "            argv[4]: if given-1 performs predictions based on given inputs.\n",
    "            argv[5]: if given-1 performs evaluations and displays them.\n",
    "    \"\"\"\n",
    "    perform_eda = int(argv[1])\n",
    "    prepare_data = int(argv[2])\n",
    "    train_mode = int(argv[3])\n",
    "    prediction_mode = int(argv[4])\n",
    "    evaluation_mode = int(argv[5])\n",
    "\n",
    "    # Paths to data.\n",
    "    train_tsv_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/WikiQACorpus/WikiQA-train.tsv\"\n",
    "    dev_tsv_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/WikiQACorpus/WikiQA-dev.tsv\"\n",
    "    test_tsv_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/WikiQACorpus/WikiQA-test.tsv\"\n",
    "\n",
    "    if perform_eda == 1:  # If given 1 by user perfrom EDA.\n",
    "        # Initialize the ExploratoryAnalysis object.\n",
    "        ob = ExploratoryAnalysis(train_tsv=train_tsv_path, dev_tsv=dev_tsv_path,\n",
    "                                 test_tsv=test_tsv_path)\n",
    "\n",
    "        ob.exploreTrainTsv()  # Performs EDA for TrainTSV.\n",
    "        ob.exploreDevTsv()  # Performs EDA for DevTSV.\n",
    "        ob.exploreTestTsv()  # Performs EDA for TestTSV\n",
    "\n",
    "    if prepare_data == 1:  # If given 1 by user prepares the data need to train the model.\n",
    "        pos_ans_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/WikiQACorpus/WikiQASent.pos.ans.tsv\"\n",
    "        # Initialize the PrepareData object.\n",
    "        ob = PrepareData(train_tsv=train_tsv_path, dev_tsv=dev_tsv_path,\n",
    "                         test_tsv=test_tsv_path, pos_ans_tsv=pos_ans_path)\n",
    "        # Preprocess the data and prepares the final data that is ready for training.\n",
    "        ob.Preprocess()\n",
    "\n",
    "    # Note: Run prepare data atleast once before running for training data to be available.\n",
    "    if train_mode == 1:  # If given 1 by user the model training begins.\n",
    "        train_df = pd.read_csv(\"data/train.csv\")  # Load the train_df.\n",
    "        dev_df = pd.read_csv(\"data/dev.csv\")  # Load the dev_df.\n",
    "        test_df = pd.read_csv(\"data/test.csv\")  # Load the test_df\n",
    "        ob = Models(train_df, dev_df, test_df)\n",
    "        ob.train_model()  # Train the model.\n",
    "\n",
    "    # Note: Run train_model atleast once before you run this.\n",
    "    if prediction_mode == 1:  # If given 1 by user prediction mode turns on.\n",
    "        loaded_model = load_model(\"Models\")\n",
    "        # Load tokenizer from fine-tuned model.\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"Models\")\n",
    "        # Initialize the Predictions object.\n",
    "        ob = Predictions(loaded_model, tokenizer)\n",
    "        question = \"How will Diaphragm Pump work\"  # Input question\n",
    "        context = \"A diaphragm pump (also known as a Membrane pump, Air Operated Double Diaphragm Pump (AODD) or Pneumatic Diaphragm Pump) is a positive displacement pump that uses a combination of the reciprocating action of a rubber , thermoplastic or teflon diaphragm and suitable valves either side of the diaphragm ( check valve , butterfly valves, flap valves, or any other form of shut-off valves) to pump a fluid .\"\n",
    "        # Make a prediction using the loaded model and tokenizer\n",
    "        answer = ob.make_prediction(question, context)\n",
    "        print(\"Answer:\", answer)\n",
    "\n",
    "    # Note: Run train_model atleast once before you run this.\n",
    "    if evaluation_mode == 1:  # If given 1 by user computes and displays the metrics.\n",
    "        loaded_model = load_model(\"Models\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"Models\")\n",
    "        train_df = pd.read_csv(\"data/train.csv\")\n",
    "        dev_df = pd.read_csv(\"data/dev.csv\")\n",
    "        test_df = pd.read_csv(\"data/test.csv\")\n",
    "        eval_df = pd.read_csv(\"data/eval.csv\")\n",
    "        ob1 = Predictions(loaded_model, tokenizer)  # Predictions object.\n",
    "        # Evaluations object.\n",
    "        ob2 = Evaluations(train_df, dev_df, test_df, eval_df,\n",
    "                          loaded_model, tokenizer, ob1)\n",
    "        # Compute predictions and store them in a csv_file.\n",
    "        ob2.compute_store_predictions()\n",
    "        pr, re, f1 = ob2.display_metrics(pd.read_csv(\n",
    "            \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/Chatbot-development/data/predictions.csv\"))\n",
    "        print(f\"train data:Precision is {pr} and recall is {re} and f1-score is {f1}\")\n",
    "        pr, re, f1 = ob2.display_metrics(pd.read_csv(\"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/Chatbot-development/data/eval_predictions.csv\"))\n",
    "        print(f\"test data:Precision is {pr} and recall is {re} and f1-score is {f1}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e73ea",
   "metadata": {},
   "source": [
    "### Order of Exegution.\n",
    "1. Run the following command in command line python main.py \"arg1\" \"arg2\" \"arg3\" \"arg4\" \"arg5\"\n",
    "* **if arg-1=1** then the code base performs exploratary data analysis and displays the results.\n",
    "* **if arg-2=1** then the code base performs data cleaning and transforms it into form which is ready to be used to fine tune the Bert model for Question Answering task.\n",
    "* **if arg-3=1** then the code base trains the model by taking the transformed data.\n",
    "* **if arg-4=1** then the code base takes a input question and context given by user and returns the predicted results.\n",
    "* **if arg-5=1** then the code base computes the token-level precision, recall, f1-score for the entire training and evaluation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3364336",
   "metadata": {},
   "source": [
    "### **step-1:** Exploratory Data Analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_eda == 1:  # If given 1 by user perfrom EDA.\n",
    "        # Initialize the ExploratoryAnalysis object.\n",
    "        ob = ExploratoryAnalysis(train_tsv=train_tsv_path, dev_tsv=dev_tsv_path,\n",
    "                                 test_tsv=test_tsv_path)\n",
    "\n",
    "        ob.exploreTrainTsv()  # Performs EDA for TrainTSV.\n",
    "        ob.exploreDevTsv()  # Performs EDA for DevTSV.\n",
    "        ob.exploreTestTsv()  # Performs EDA for TestTSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9061a3",
   "metadata": {},
   "source": [
    "* **As shown in the above code snippet it first initialized the object of ExploratoryDataAnalysis class with paths of train, dev, test data sets and calls functions which displays the results for each of train, dev, test sets.**\n",
    "\n",
    "* **The ExloratoryAnalysis class in present in the Exploratory_data_analysis.py file given below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f931a",
   "metadata": {},
   "source": [
    "# Exploratory_data_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Jyothi Vishnu Vardhan Kolla.\n",
    "\n",
    "This file contains code to perform exporatory data analysis.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ExploratoryAnalysis:\n",
    "    def __init__(self, train_tsv: str, dev_tsv: str, test_tsv: str):\n",
    "        \"\"\"\n",
    "        Initializes the train, dev, test paths with class variables.\n",
    "\n",
    "        ARGS:\n",
    "            train_tsv: Path to train_tsv file.\n",
    "            dev_tsv: Path to dev_tsv file.\n",
    "            test_tsv: Path to test_tsv file.\n",
    "        \"\"\"\n",
    "        self.train_tsv_path = train_tsv\n",
    "        self.dev_tsv_path = dev_tsv\n",
    "        self.test_tsv_path = test_tsv\n",
    "\n",
    "    def exploreData(self, data_path):\n",
    "        # Read and display the file as pandas dataFrame.\n",
    "        df = pd.read_csv(data_path, delimiter='\\t')\n",
    "        print(df.head(5))\n",
    "\n",
    "        # Get the number of data in the data.\n",
    "        print(f\"The number of data points in train_tsv file are {df.shape[0]}\")\n",
    "\n",
    "        # Check for the number of questions.\n",
    "        unique_questions = df['QuestionID'].unique()\n",
    "        print(f\"The number of unique questions are  {len(unique_questions)}\")\n",
    "\n",
    "        # check the count of no.of documents.\n",
    "        no_of_documents = df['DocumentID'].unique()\n",
    "        print(\n",
    "            f\"The number of documents in train_tsv file is {len(no_of_documents)}\")\n",
    "\n",
    "        # find the average no.of questions per document.\n",
    "        avg_questions_per_doc = df.groupby(\n",
    "            'DocumentID')['QuestionID'].nunique().mean()\n",
    "        # Print the result\n",
    "        print(\"Average number of unique QuestionID values per DocumentID:\",\n",
    "              avg_questions_per_doc)\n",
    "\n",
    "        # get list of documents with more than one question.\n",
    "        docs_with_multiple_questions = df.groupby('DocumentID').filter(\n",
    "            lambda x: x['QuestionID'].nunique() > 1)['DocumentID'].unique()\n",
    "        print(docs_with_multiple_questions)\n",
    "        print(\n",
    "            f\"No of documents with more than one question is {len(docs_with_multiple_questions)} which is {len(no_of_documents) / len(docs_with_multiple_questions)}%\")\n",
    "\n",
    "        # Find the average number of answers per question.\n",
    "        average_no_of_answers_per_question = df.groupby(\n",
    "            'QuestionID')['SentenceID'].nunique().mean()\n",
    "        print(\n",
    "            f\"Average no.of answers per question {average_no_of_answers_per_question}\")\n",
    "\n",
    "    def exploreTrainTsv(self):\n",
    "        print(f\"\\nExploratory DataAnalysis of train_tsv data\")\n",
    "        self.exploreData(self.train_tsv_path)\n",
    "\n",
    "    def exploreDevTsv(self):\n",
    "        print(f\"\\nExploratory DataAnalysis of dev_tsv data\")\n",
    "        self.exploreData(self.dev_tsv_path)\n",
    "\n",
    "    def exploreTestTsv(self):\n",
    "        print(f\"\\nExploratory DataAnalysis of test_tsv data\")\n",
    "        self.exploreData(self.test_tsv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4b634",
   "metadata": {},
   "source": [
    "### Obtained Results are as follows:\n",
    "<pre>\n",
    "Exploratory DataAnalysis of train_tsv data\n",
    "\n",
    "  QuestionID                       Question DocumentID DocumentTitle SentenceID                                           Sentence  Label\n",
    "0         Q1  how are glacier caves formed?         D1  Glacier cave       D1-0  A partly submerged glacier cave on Perito More...      0\n",
    "1         Q1  how are glacier caves formed?         D1  Glacier cave       D1-1          The ice facade is approximately 60 m high      0\n",
    "2         Q1  how are glacier caves formed?         D1  Glacier cave       D1-2          Ice formations in the Titlis glacier cave      0\n",
    "3         Q1  how are glacier caves formed?         D1  Glacier cave       D1-3  A glacier cave is a cave formed within the ice...      1\n",
    "4         Q1  how are glacier caves formed?         D1  Glacier cave       D1-4  Glacier caves are often called ice caves , but...      0\n",
    "\n",
    "The number of data points in train_tsv file are 20347\n",
    "The number of unique questions are  2117\n",
    "The number of documents in train_tsv file is 1994\n",
    "\n",
    "Average number of unique QuestionID values per DocumentID: 1.0616850551654964\n",
    "['D6' 'D27' 'D31' 'D41' 'D50' 'D89' 'D100' 'D124' 'D125' 'D128' 'D129'\n",
    " 'D134' 'D155' 'D160' 'D169' 'D179' 'D195' 'D211' 'D212' 'D215' 'D219'\n",
    " 'D230' 'D251' 'D252' 'D255' 'D269' 'D275' 'D292' 'D309' 'D320' 'D342'\n",
    " 'D351' 'D352' 'D359' 'D363' 'D368' 'D370' 'D371' 'D377' 'D380' 'D432'\n",
    " 'D438' 'D449' 'D473' 'D521' 'D528' 'D543' 'D549' 'D557' 'D587' 'D589'\n",
    " 'D607' 'D624' 'D698' 'D729' 'D763' 'D771' 'D773' 'D806' 'D855' 'D857'\n",
    " 'D216' 'D881' 'D885' 'D484' 'D979' 'D1016' 'D1018' 'D1054' 'D1055'\n",
    " 'D1065' 'D1081' 'D1111' 'D1142' 'D1148' 'D1158' 'D1196' 'D1197' 'D1198'\n",
    " 'D1207' 'D1263' 'D1282' 'D1298' 'D1302' 'D1308' 'D1318' 'D1454' 'D1455'\n",
    " 'D1525' 'D1566' 'D1616' 'D1621' 'D1652' 'D660' 'D1703' 'D1832' 'D1855'\n",
    " 'D1868' 'D1876' 'D1559' 'D332' 'D2045' 'D2065' 'D2089' 'D2141' 'D2174'\n",
    " 'D2212' 'D2319' 'D2452' 'D2477']\n",
    " \n",
    "No of documents with more than one question is 110 which is 18.12727272727273%\n",
    "Average no.of answers per question 9.611242324043458\n",
    "\n",
    "Exploratory DataAnalysis of dev_tsv data\n",
    "\n",
    "  QuestionID                                     Question DocumentID     DocumentTitle SentenceID                                           Sentence  Label\n",
    "0         Q8  How are epithelial tissues joined together?         D8  Tissue (biology)       D8-0  Cross section of sclerenchyma fibers in plant ...      0\n",
    "1         Q8  How are epithelial tissues joined together?         D8  Tissue (biology)       D8-1  Microscopic view of a histologic specimen of h...      0\n",
    "2         Q8  How are epithelial tissues joined together?         D8  Tissue (biology)       D8-2  In Biology , Tissue is a cellular organization...      0\n",
    "3         Q8  How are epithelial tissues joined together?         D8  Tissue (biology)       D8-3  A tissue is an ensemble of similar cells from ...      0\n",
    "4         Q8  How are epithelial tissues joined together?         D8  Tissue (biology)       D8-4  Organs are then formed by the functional group...      0\n",
    "\n",
    "The number of data points in train_tsv file are 2733\n",
    "The number of unique questions are  296\n",
    "The number of documents in train_tsv file is 293\n",
    "Average number of unique QuestionID values per DocumentID: 1.0102389078498293\n",
    "['D318' 'D1008' 'D1361']\n",
    "No of documents with more than one question is 3 which is 97.66666666666667%\n",
    "Average no.of answers per question 9.233108108108109\n",
    "\n",
    "Exploratory DataAnalysis of test_tsv data\n",
    "\n",
    "  QuestionID                                         Question DocumentID  ... SentenceID                                           Sentence Label\n",
    "0         Q0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US         D0  ...       D0-0  African immigration to the United States refer...     0\n",
    "1         Q0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US         D0  ...       D0-1  The term African in the scope of this article ...     0\n",
    "2         Q0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US         D0  ...       D0-2  From the Immigration and Nationality Act of 19...     0\n",
    "3         Q0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US         D0  ...       D0-3  African immigrants in the United States come f...     0\n",
    "4         Q0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US         D0  ...       D0-4  They include people from different national, l...     0\n",
    "\n",
    "[5 rows x 7 columns]\n",
    "\n",
    "The number of data points in train_tsv file are 6116\n",
    "The number of unique questions are  630\n",
    "The number of documents in train_tsv file is 616\n",
    "Average number of unique QuestionID values per DocumentID: 1.0227272727272727\n",
    "['D186' 'D270' 'D330' 'D332' 'D660' 'D878' 'D996' 'D1035' 'D1241' 'D1349'\n",
    " 'D1764' 'D2097']\n",
    "No of documents with more than one question is 12 which is 51.333333333333336%\n",
    "Average no.of answers per question 9.707936507936507\n",
    "(tensorflow-gpu) jyothivishnuvardhankolla@Jyothis-MacBook-Pro Chatbot-development % \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa0597c",
   "metadata": {},
   "source": [
    "# **Step-2:** Data cleaning and preparation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d17f2",
   "metadata": {},
   "source": [
    "1. **Remove all the data points which have a sentence which do not contain correct answer.**\n",
    "2. **Get all unique questions in each of the data-split.**\n",
    "3. **Merge all the data-splits with all annotated answers from annotated dataset to create a final dataset with all the valid datapoints.**\n",
    "4. **Create the final valid dataset suitable for training by creating a dataframe with columns Question, Sentence, Answer**\n",
    "\n",
    "#### Key observations:\n",
    "* **There are a total of 1039 questions in the train_data with a proper answer.**\n",
    "* **There are a total of 140 questions in the train_data with a proper answer.**\n",
    "* **There are a total of 291 questions in the train_data with a proper answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prepare_data == 1:  # If given 1 by user prepares the data need to train the model.\n",
    "        pos_ans_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/WikiQACorpus/WikiQASent.pos.ans.tsv\"\n",
    "        # Initialize the PrepareData object.\n",
    "        ob = PrepareData(train_tsv=train_tsv_path, dev_tsv=dev_tsv_path,\n",
    "                         test_tsv=test_tsv_path, pos_ans_tsv=pos_ans_path)\n",
    "        # Preprocess the data and prepares the final data that is ready for training.\n",
    "        ob.Preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0376b",
   "metadata": {},
   "source": [
    "**As shown above it takes the path to annotated datasets and Initializes the object of PrepareData class from Data_prep.py file, then calls Preprocess function which performs all the cleaning and transformations required for fine-tuning the model as shown below**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab304ca",
   "metadata": {},
   "source": [
    "# Data_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eedbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class PrepareData:\n",
    "    def __init__(self, train_tsv: str, dev_tsv: str, test_tsv: str, pos_ans_tsv: str):\n",
    "        \"\"\"\n",
    "        Creates the Necessary datframes required to create the final_data,\n",
    "        Initialize the object of this class with following args and then call \n",
    "        preprocess function to prepare your data.\n",
    "\n",
    "        ARGS:\n",
    "            train_tsv: Path to train_tsv file.\n",
    "            dev_tsv: Path to dev_tsv file.\n",
    "            test_tsv: Path to test_tsv file.\n",
    "        \"\"\"\n",
    "        self.train_tsv_path = train_tsv\n",
    "        self.dev_tsv_path = dev_tsv\n",
    "        self.test_tsv_path = test_tsv\n",
    "        self.pos_ans_tsv = pos_ans_tsv\n",
    "\n",
    "        # Initialize variables to store dataframes.\n",
    "        self.pos_ans_tsv_df = None\n",
    "        self.train_tsv_df = None\n",
    "        self.dev_tsv_df = None\n",
    "        self.test_tsv_df = None\n",
    "\n",
    "        # Initialize variables to store final train, dev, test dataframes.\n",
    "        self.final_train_df = None\n",
    "        self.final_dev_df = None\n",
    "        self.final_test_df = None\n",
    "\n",
    "    def create_data_frames(self):\n",
    "        # Creates the train, dev, test, pos_ans dataframes.\n",
    "        self.pos_ans_tsv_df = pd.read_csv(self.pos_ans_tsv, delimiter=\"\\t\")\n",
    "        self.train_tsv_df = pd.read_csv(self.train_tsv_path, delimiter=\"\\t\")\n",
    "        self.dev_tsv_df = pd.read_csv(self.dev_tsv_path, delimiter=\"\\t\")\n",
    "        self.test_tsv_df = pd.read_csv(self.test_tsv_path, delimiter=\"\\t\")\n",
    "\n",
    "    def create_final_df(self, df: pd.DataFrame):\n",
    "        # Takes in a dataframe and creates a new df ready for training\n",
    "        final_dict = {\n",
    "            'question': [],\n",
    "            'sentence': [],\n",
    "            'answer': []\n",
    "        }\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            # Iterate through each row and create new row for each of correct answer.\n",
    "            if pd.notna(row['AnswerPhrase1']) and row['AnswerPhrase1'] != 'NO_ANS':\n",
    "                final_dict['question'].append(row['Question'])\n",
    "                final_dict['sentence'].append(row['Sentence'])\n",
    "                final_dict['answer'].append(row['AnswerPhrase1'])\n",
    "\n",
    "            if pd.notna(row['AnswerPhrase2']) and row['AnswerPhrase2'] != 'NO_ANS':\n",
    "                final_dict['question'].append(row['Question'])\n",
    "                final_dict['sentence'].append(row['Sentence'])\n",
    "                final_dict['answer'].append(row['AnswerPhrase2'])\n",
    "\n",
    "            if pd.notna(row['AnswerPhrase3']) and row['AnswerPhrase3'] != 'NO_ANS':\n",
    "                final_dict['question'].append(row['Question'])\n",
    "                final_dict['sentence'].append(row['Sentence'])\n",
    "                final_dict['answer'].append(row['AnswerPhrase3'])\n",
    "\n",
    "        return pd.DataFrame(final_dict)\n",
    "\n",
    "    def Preprocess(self):\n",
    "        # Creates the final cleaned datasets and stored them in disk.\n",
    "        self.create_data_frames()  # Create dataframes.\n",
    "        # Preprocess the dataframes(eliminate all rows with incorrect sentences.)\n",
    "        cleaned_train_tsv_data = self.train_tsv_df[self.train_tsv_df[\"Label\"] == 1]\n",
    "        cleaned_dev_tsv_data = self.dev_tsv_df[self.dev_tsv_df[\"Label\"] == 1]\n",
    "        cleaned_test_tsv_data = self.test_tsv_df[self.test_tsv_df[\"Label\"] == 1]\n",
    "\n",
    "        # Get list of all unique_questions in the cleaned data.\n",
    "        unique_questions_train = cleaned_train_tsv_data['QuestionID'].unique()\n",
    "        unique_questions_dev = cleaned_dev_tsv_data['QuestionID'].unique()\n",
    "        unique_questions_test = cleaned_test_tsv_data['QuestionID'].unique()\n",
    "\n",
    "        # separate train_pos_ans_df for each of train, dev, test.\n",
    "        train_pos_ans_tsv = self.pos_ans_tsv_df[self.pos_ans_tsv_df['QuestionID'].isin(\n",
    "            unique_questions_train)]\n",
    "        dev_pos_ans_tsv = self.pos_ans_tsv_df[self.pos_ans_tsv_df['QuestionID'].isin(\n",
    "            unique_questions_dev)]\n",
    "        test_pos_ans_tsv = self.pos_ans_tsv_df[self.pos_ans_tsv_df['QuestionID'].isin(\n",
    "            unique_questions_test)]\n",
    "\n",
    "        # Merge train, dev, test dataframes with pos_ans dataframes.\n",
    "        merged_train_df = pd.merge(train_pos_ans_tsv, cleaned_train_tsv_data, on=[\n",
    "                                   \"QuestionID\", \"Question\", \"DocumentID\", \"DocumentTitle\", \"SentenceID\", \"Sentence\"])\n",
    "        merged_dev_df = pd.merge(dev_pos_ans_tsv, cleaned_dev_tsv_data, on=[\n",
    "                                 \"QuestionID\", \"Question\", \"DocumentID\", \"DocumentTitle\", \"SentenceID\", \"Sentence\"])\n",
    "        merged_test_df = pd.merge(test_pos_ans_tsv, cleaned_test_tsv_data, on=[\n",
    "                                  \"QuestionID\", \"Question\", \"DocumentID\", \"DocumentTitle\", \"SentenceID\", \"Sentence\"])\n",
    "\n",
    "        # Create the final dataframes.\n",
    "        self.final_train_df = self.create_final_df(merged_train_df)\n",
    "        self.final_dev_df = self.create_final_df(merged_dev_df)\n",
    "        self.final_test_df = self.create_final_df(merged_test_df)\n",
    "\n",
    "        # Save the final dataframes in a csv file.\n",
    "        self.final_train_df.to_csv(\"data/train.csv\", index=False)\n",
    "        self.final_dev_df.to_csv(\"data/dev.csv\", index=False)\n",
    "        self.final_test_df.to_csv(\"data/test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf1e97",
   "metadata": {},
   "source": [
    "**The final data-set contains a total of 1941 data points with Question,Sentence,Answer columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ecd5c",
   "metadata": {},
   "source": [
    "# Step-3:Model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1ef520",
   "metadata": {},
   "source": [
    "**I have used Fine-tuned Bert to build the question-answering system.**\n",
    "1. First lets discuss about the Bert model which is basically a trained Transformer encoded stack.\n",
    "![example image](Bert_training.png \"Example Image\")\n",
    "2. As shown in the above picture Bert is first trained in a Semi-Supervised setting on large amount of data and then it can be used for various NLP tasks such as classification, Question-Answering etc.\n",
    "3. Bert has a total of two variants which are Bert-Base, Bert-Large where Bert-base has 12 attention heads and Bert-large has 16 attention heads with 768 and 1024 hidden units respectively.\n",
    "\n",
    "**Training Process for Bert**\n",
    "![example image](Bert-Model.png \"Example Image\")\n",
    "1. The First Input is always supplied with a special [CLS] token and each encoder layer will itself contain one Self-attention and its outputs are passed through a feed-forward network.\n",
    "2. One of the two important features of Bert are Masked Language Model(Randomly mask 15% of tokens) and Two-sentence Tasks (Given two sentences A and B is B likely to be sentence that follows A?)\n",
    "3. This is how we modify and Fine-Tune bert for Question-Answering task as shown in the Bert official paper.\n",
    "![example image](Bert-QA.png \"Example Image\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f3ac07",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Note: Run prepare data atleast once before running for training data to be available.\n",
    "    if train_mode == 1:  # If given 1 by user the model training begins.\n",
    "        train_df = pd.read_csv(\"data/train.csv\")  # Load the train_df.\n",
    "        dev_df = pd.read_csv(\"data/dev.csv\")  # Load the dev_df.\n",
    "        test_df = pd.read_csv(\"data/test.csv\")  # Load the test_df\n",
    "        ob = Models(train_df, dev_df, test_df)\n",
    "        ob.train_model()  # Train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdc0aaa",
   "metadata": {},
   "source": [
    "* As shown in the above code snippet it first initialized the object of Models class with paths of train, dev, test data sets and calls functions which trains the fine-tuned bert-base for 10 epochs.\n",
    "* The Model class in present in models.py file given below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00cf6d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Jyothi Vishnu Vardhan Kolla.\n",
    "\n",
    "This files contains the code to train and build models.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer\n",
    "from transformers import TFBertModel\n",
    "from utils import preprocess_data\n",
    "from transformers import CONFIG_MAPPING\n",
    "\n",
    "\n",
    "class Models:\n",
    "    def __init__(self, train_data=None, dev_data=None, test_data=None):\n",
    "        \"\"\"\n",
    "        Takes the training_data, dev_data, test_data into one\n",
    "        final_data and trains the model.\n",
    "\n",
    "        Args:\n",
    "            train_data: training dataframe.\n",
    "            dev_data: dev_dataframe.\n",
    "            test_data: testing_dataframe.\n",
    "        \"\"\"\n",
    "        self.train_data = train_data\n",
    "        self.dev_data = dev_data\n",
    "        self.test_data = test_data\n",
    "\n",
    "        # Combine the three data frames into a single dataframe.\n",
    "        self.final_data = pd.concat(\n",
    "            [self.train_data, self.dev_data, self.test_data], ignore_index=True)\n",
    "        self.sequence_length = 384\n",
    "        # Load pretrained tokenizer from bert model.\n",
    "        self.tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    def prepare_data(self):\n",
    "        # Prepare the x_train and y_train splits to make it ready for training.\n",
    "        x_train, y_train = preprocess_data(questions=self.final_data['question'],\n",
    "                                           sentences=self.final_data['sentence'],\n",
    "                                           answers=self.final_data['answer'],\n",
    "                                           tokenizer=self.tokenizer,\n",
    "                                           seq_length=self.sequence_length)\n",
    "        return x_train, y_train\n",
    "\n",
    "    def create_qa_model(self):\n",
    "        # Finetunes the Bert model for question-answering task.\n",
    "        input_ids = tf.keras.layers.Input(\n",
    "            shape=(self.sequence_length, ), dtype=tf.int32, name='input_ids')\n",
    "        attention_mask = tf.keras.layers.Input(\n",
    "            shape=(self.sequence_length,), dtype=tf.int32, name=\"attention_mask\")\n",
    "\n",
    "        bert = TFBertModel.from_pretrained(\n",
    "            'bert-base-uncased', return_dict=True)\n",
    "        sequence_output = bert([input_ids, attention_mask])[\n",
    "            'last_hidden_state']\n",
    "\n",
    "        start_logits = tf.keras.layers.Dense(\n",
    "            1, name='start_position')(sequence_output)\n",
    "        # output layer to predict first_idx of answer.\n",
    "        start_logits = tf.keras.layers.Flatten()(start_logits)\n",
    "\n",
    "        end_logits = tf.keras.layers.Dense(\n",
    "            1, name='end_position')(sequence_output)\n",
    "        # output layer to predict last_idx of answer.\n",
    "        end_logits = tf.keras.layers.Flatten()(end_logits)\n",
    "\n",
    "        model = tf.keras.Model(inputs=[input_ids, attention_mask], outputs=[\n",
    "                               start_logits, end_logits])\n",
    "        return model\n",
    "\n",
    "    def train_model(self):\n",
    "        # trains the fine-tuned model for qa_task,\n",
    "        x_train, y_train = self.prepare_data()\n",
    "        qa_model = self.create_qa_model()\n",
    "\n",
    "        optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=5e-5)\n",
    "        loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "        qa_model.compile(optimizer=optimizer, loss=loss, metrics=['accuracy'])\n",
    "\n",
    "        epochs = 10\n",
    "        batch_size = 8\n",
    "\n",
    "        history = qa_model.fit(x_train, [y_train['start_position'],\n",
    "                                         y_train['end_position']], epochs=epochs, batch_size=batch_size)\n",
    "\n",
    "        model_save_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/Chatbot-development/Models\"\n",
    "\n",
    "        # Save the model weights\n",
    "        qa_model.save_weights(model_save_path + \"/tf_model.h5\")\n",
    "\n",
    "        # Save the tokenizer\n",
    "        self.tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "        # Save the config\n",
    "        config = CONFIG_MAPPING[\"bert\"]()\n",
    "        config.save_pretrained(model_save_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d634dcd",
   "metadata": {},
   "source": [
    "### Reasons for selecting Bert model and not selecting models such as GPT and T5\n",
    "1. **Bidirectional Context:** Bert is designed to capture context in both left and right sides of the token, thus this bidirectional context in bert allows it to better understand the contextual relation between words in a sentence. As explained in Bert paper GPT process the text in uni-directional way which limits its ability to understand context in text and more suitable for text generation tasks. Although T5 is bi-directional it is not as optimized as bert for bidirectional context representation.\n",
    "\n",
    "2. **Pre-training Objectives:** Masked Language Modelling and Next Sentence Prediction methods in Pre-training Bert makes Bert better understand language and context which is more useful for tasks like question answering while on contrast GPT uses a uni-directional modelling objective while T5 uses denoising autoencoder objective.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8c1fb1",
   "metadata": {},
   "source": [
    "### Evaluation Metrics For Question Answering\n",
    "**How can we evaluate how better our model is performing with help of metrics such as Accuracy, Precision, Recall, F1-Score.**\n",
    "\n",
    "* As we are dealing with Question-Answering which is just as same as classification predicting the first and last index of the answer in sentence but as per the Bert paper they used a slightly modified version of these metrics to evaluate their model.\n",
    "* Accuracy -> We evaluate Accuracy as follows we compute accuracy for both the first_index and last_index predictions.\n",
    "* Precision -> It is caluculated as (Number of correct tokens in the predicted answer) / (Total number of tokens in the predicted answer).\n",
    "* Recall -> It is caluculated as (Number of correct tokens in the predicted answer) / (Total number of tokens in the actual answer).\n",
    "* F1-score -> 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "**In this way these metrics are computed for every data point in the dataset and then average values are reported as the overall performance of the model.**\n",
    "\n",
    "Let's understand this with an example.<br>\n",
    "Actual Answer: \"Paris\"<br>\n",
    "Predicted Answer: \"Paris city\"<br>\n",
    "\n",
    "Now let's calculate precision, recall, and F1-score for this example:<br>\n",
    "\n",
    "**Precision:**<br>\n",
    "Number of correct tokens in the predicted answer: 1 (\"Paris\")<br>\n",
    "Total number of tokens in the predicted answer: 2 (\"Paris\" and \"city\")<br>\n",
    "Precision = 1 (correct token) / 2 (total tokens in the predicted answer) = 0.5<br>\n",
    "\n",
    "**Recall:**<br>\n",
    "Number of correct tokens in the predicted answer: 1 (\"Paris\")<br>\n",
    "Total number of tokens in the actual answer: 1 (\"Paris\")<br>\n",
    "Recall = 1 (correct token) / 1 (total tokens in the actual answer) = 1<br>\n",
    "\n",
    "**F1-score:**<br>\n",
    "Precision = 0.5<br>\n",
    "Recall = 1<br>\n",
    "F1-score = 2 * (0.5 * 1) / (0.5 + 1) = 2 / 1.5 = 4 / 3 â‰ˆ 0.67<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2d698df",
   "metadata": {},
   "outputs": [],
   "source": [
    "if evaluation_mode == 1:  # If given 1 by user computes and displays the metrics.\n",
    "        loaded_model = load_model(\"Models\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"Models\")\n",
    "        train_df = pd.read_csv(\"data/train.csv\")\n",
    "        dev_df = pd.read_csv(\"data/dev.csv\")\n",
    "        test_df = pd.read_csv(\"data/test.csv\")\n",
    "        eval_df = pd.read_csv(\"data/eval.csv\")\n",
    "        ob1 = Predictions(loaded_model, tokenizer)  # Predictions object.\n",
    "        # Evaluations object.\n",
    "        ob2 = Evaluations(train_df, dev_df, test_df, eval_df,\n",
    "                          loaded_model, tokenizer, ob1)\n",
    "        # Compute predictions and store them in a csv_file.\n",
    "        ob2.compute_store_predictions()\n",
    "        pr, re, f1 = ob2.display_metrics(pd.read_csv(\n",
    "            \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/Chatbot-development/data/predictions.csv\"))\n",
    "        print(f\"train data:Precision is {pr} and recall is {re} and f1-score is {f1}\")\n",
    "        pr, re, f1 = ob2.display_metrics(pd.read_csv(\"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/Chatbot-development/data/eval_predictions.csv\"))\n",
    "        print(f\"test data:Precision is {pr} and recall is {re} and f1-score is {f1}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700b2c6",
   "metadata": {},
   "source": [
    "**1.As shown in the above code snippet it first loads the fine-tuned model, tokenizer, training_data and Predictions, Evaluations classes object which are in predictions.py file and displays the metrics on training and testing data.**<br>\n",
    "\n",
    "**2.Then this calls relevant functions as shown above and displays the overall metrics**<br>\n",
    "\n",
    "**Accuracies on train data:** start_idx: 72.08%; end_idx: 84.56%<br>\n",
    "**Precision on train data:** 0.7901782329047791<br>\n",
    "**Recall on train data:** 0.8227536900992919<br>\n",
    "**F1-score on train data:** 0.7786718907778672<br>\n",
    "\n",
    "**Precision on test data:** 0.5045234018326576<br>\n",
    "**Recall on test data:** 0.5785156116685997 <br>\n",
    "**F1-score on test data:** 0.501535388012117 <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062a86a6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
