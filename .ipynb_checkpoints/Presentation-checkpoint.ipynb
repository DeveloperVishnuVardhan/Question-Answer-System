{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f36a6898",
   "metadata": {},
   "source": [
    "## **Task:** Fine-tune a pre-trained Transformer-based chatbot using Tensorflow on the WikiQA corpus dataset.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7368e6",
   "metadata": {},
   "source": [
    "### Approach:\n",
    "1. Exploratory Data Analysis.\n",
    "2. Data cleaning and preparation.\n",
    "3. Model training.\n",
    "4. Testing the model.\n",
    "5. Performing evaluations on Train and Test data.\n",
    "\n",
    "### The pipleline of the system is as follows:\n",
    "* **main.py:** The main file which performs all the tasks from exploratory data analysis, data-preparation, model-training, predictions, evaluations based on command line arguements given by user.\n",
    "* **Data_prep.py:** Contains the code and functions needed to clean all the train_data and transform the data which is ready to be feeded to the model.\n",
    "* **model.py:** This file contains the code to use the transformed data to fine-tune the bert model for a question answering task.\n",
    "* **Predictions.py:** This file contains all the code to perform predictions and evaluations on the testing data and example inputs given by user.\n",
    "* **utils.py:** This file contains all the helper_functions used in the pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f5f47e",
   "metadata": {},
   "source": [
    "# main.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcacdfa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Jyothi Vishnu Vardhan Kolla.\n",
    "\n",
    "This is the main file which performs tasks such as\n",
    "explorataroy data anaysis,data-preparation,model-training based on command line inputs.\n",
    "\"\"\"\n",
    "import sys\n",
    "from Exploratory_data_analysis import ExploratoryAnalysis\n",
    "from Data_prep import PrepareData\n",
    "from models import Models\n",
    "import pandas as pd\n",
    "from transformers import BertConfig, BertTokenizer\n",
    "from utils import load_model\n",
    "from Predictions import Predictions, Evaluations\n",
    "\n",
    "\n",
    "def main(argv):\n",
    "    \"\"\"\n",
    "        Main which takes command line arguements and executes the pipeline.\n",
    "\n",
    "        ARGS:\n",
    "            argv[1]: if given-1 performs exploratory data analysis on given data.\n",
    "            argv[2]: if given-1 performing preprocessing and prepares the final data.\n",
    "            argv[3]: if given-1 performs the training.\n",
    "            argv[4]: if given-1 performs predictions based on given inputs.\n",
    "            argv[5]: if given-1 performs evaluations and displays them.\n",
    "    \"\"\"\n",
    "    perform_eda = int(argv[1])\n",
    "    prepare_data = int(argv[2])\n",
    "    train_mode = int(argv[3])\n",
    "    prediction_mode = int(argv[4])\n",
    "    evaluation_mode = int(argv[5])\n",
    "\n",
    "    # Paths to data.\n",
    "    train_tsv_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/WikiQACorpus/WikiQA-train.tsv\"\n",
    "    dev_tsv_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/WikiQACorpus/WikiQA-dev.tsv\"\n",
    "    test_tsv_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/WikiQACorpus/WikiQA-test.tsv\"\n",
    "\n",
    "    if perform_eda == 1:  # If given 1 by user perfrom EDA.\n",
    "        # Initialize the ExploratoryAnalysis object.\n",
    "        ob = ExploratoryAnalysis(train_tsv=train_tsv_path, dev_tsv=dev_tsv_path,\n",
    "                                 test_tsv=test_tsv_path)\n",
    "\n",
    "        ob.exploreTrainTsv()  # Performs EDA for TrainTSV.\n",
    "        ob.exploreDevTsv()  # Performs EDA for DevTSV.\n",
    "        ob.exploreTestTsv()  # Performs EDA for TestTSV\n",
    "\n",
    "    if prepare_data == 1:  # If given 1 by user prepares the data need to train the model.\n",
    "        pos_ans_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/WikiQACorpus/WikiQASent.pos.ans.tsv\"\n",
    "        # Initialize the PrepareData object.\n",
    "        ob = PrepareData(train_tsv=train_tsv_path, dev_tsv=dev_tsv_path,\n",
    "                         test_tsv=test_tsv_path, pos_ans_tsv=pos_ans_path)\n",
    "        # Preprocess the data and prepares the final data that is ready for training.\n",
    "        ob.Preprocess()\n",
    "\n",
    "    # Note: Run prepare data atleast once before running for training data to be available.\n",
    "    if train_mode == 1:  # If given 1 by user the model training begins.\n",
    "        train_df = pd.read_csv(\"data/train.csv\")  # Load the train_df.\n",
    "        dev_df = pd.read_csv(\"data/dev.csv\")  # Load the dev_df.\n",
    "        test_df = pd.read_csv(\"data/test.csv\")  # Load the test_df\n",
    "        ob = Models(train_df, dev_df, test_df)\n",
    "        ob.train_model()  # Train the model.\n",
    "\n",
    "    # Note: Run train_model atleast once before you run this.\n",
    "    if prediction_mode == 1:  # If given 1 by user prediction mode turns on.\n",
    "        loaded_model = load_model(\"Models\")\n",
    "        # Load tokenizer from fine-tuned model.\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"Models\")\n",
    "        # Initialize the Predictions object.\n",
    "        ob = Predictions(loaded_model, tokenizer)\n",
    "        question = \"How will Diaphragm Pump work\"  # Input question\n",
    "        context = \"A diaphragm pump (also known as a Membrane pump, Air Operated Double Diaphragm Pump (AODD) or Pneumatic Diaphragm Pump) is a positive displacement pump that uses a combination of the reciprocating action of a rubber , thermoplastic or teflon diaphragm and suitable valves either side of the diaphragm ( check valve , butterfly valves, flap valves, or any other form of shut-off valves) to pump a fluid .\"\n",
    "        # Make a prediction using the loaded model and tokenizer\n",
    "        answer = ob.make_prediction(question, context)\n",
    "        print(\"Answer:\", answer)\n",
    "\n",
    "    # Note: Run train_model atleast once before you run this.\n",
    "    if evaluation_mode == 1:  # If given 1 by user computes and displays the metrics.\n",
    "        loaded_model = load_model(\"Models\")\n",
    "        tokenizer = BertTokenizer.from_pretrained(\"Models\")\n",
    "        train_df = pd.read_csv(\"data/train.csv\")\n",
    "        dev_df = pd.read_csv(\"data/dev.csv\")\n",
    "        test_df = pd.read_csv(\"data/test.csv\")\n",
    "        eval_df = pd.read_csv(\"data/eval.csv\")\n",
    "        ob1 = Predictions(loaded_model, tokenizer)  # Predictions object.\n",
    "        # Evaluations object.\n",
    "        ob2 = Evaluations(train_df, dev_df, test_df, eval_df,\n",
    "                          loaded_model, tokenizer, ob1)\n",
    "        # Compute predictions and store them in a csv_file.\n",
    "        ob2.compute_store_predictions()\n",
    "        pr, re, f1 = ob2.display_metrics(pd.read_csv(\n",
    "            \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/Chatbot-development/data/predictions.csv\"))\n",
    "        print(f\"train data:Precision is {pr} and recall is {re} and f1-score is {f1}\")\n",
    "        pr, re, f1 = ob2.display_metrics(pd.read_csv(\"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/Chatbot-development/data/eval_predictions.csv\"))\n",
    "        print(f\"test data:Precision is {pr} and recall is {re} and f1-score is {f1}\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main(sys.argv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "648e73ea",
   "metadata": {},
   "source": [
    "### Order of Exegution.\n",
    "1. Run the following command in command line python main.py \"arg1\" \"arg2\" \"arg3\" \"arg4\" \"arg5\"\n",
    "* **if arg-1=1** then the code base performs exploratary data analysis and displays the results.\n",
    "* **if arg-2=1** then the code base performs data cleaning and transforms it into form which is ready to be used to fine tune the Bert model for Question Answering task.\n",
    "* **if arg-3=1** then the code base trains the model by taking the transformed data.\n",
    "* **if arg-4=1** then the code base takes a input question and context given by user and returns the predicted results.\n",
    "* **if arg-5=1** then the code base computes the token-level precision, recall, f1-score for the entire training and evaluation datasets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3364336",
   "metadata": {},
   "source": [
    "### **step-1:** Exploratory Data Analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e2e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if perform_eda == 1:  # If given 1 by user perfrom EDA.\n",
    "        # Initialize the ExploratoryAnalysis object.\n",
    "        ob = ExploratoryAnalysis(train_tsv=train_tsv_path, dev_tsv=dev_tsv_path,\n",
    "                                 test_tsv=test_tsv_path)\n",
    "\n",
    "        ob.exploreTrainTsv()  # Performs EDA for TrainTSV.\n",
    "        ob.exploreDevTsv()  # Performs EDA for DevTSV.\n",
    "        ob.exploreTestTsv()  # Performs EDA for TestTSV"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac9061a3",
   "metadata": {},
   "source": [
    "* **As shown in the above code snippet it first initialized the object of ExploratoryDataAnalysis class with paths of train, dev, test data sets and calls functions which displays the results for each of train, dev, test sets.**\n",
    "\n",
    "* **The ExloratoryAnalysis class in present in the Exploratory_data_analysis.py file given below.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "099f931a",
   "metadata": {},
   "source": [
    "# Exploratory_data_analysis.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b621ca88",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Jyothi Vishnu Vardhan Kolla.\n",
    "\n",
    "This file contains code to perform exporatory data analysis.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class ExploratoryAnalysis:\n",
    "    def __init__(self, train_tsv: str, dev_tsv: str, test_tsv: str):\n",
    "        \"\"\"\n",
    "        Initializes the train, dev, test paths with class variables.\n",
    "\n",
    "        ARGS:\n",
    "            train_tsv: Path to train_tsv file.\n",
    "            dev_tsv: Path to dev_tsv file.\n",
    "            test_tsv: Path to test_tsv file.\n",
    "        \"\"\"\n",
    "        self.train_tsv_path = train_tsv\n",
    "        self.dev_tsv_path = dev_tsv\n",
    "        self.test_tsv_path = test_tsv\n",
    "\n",
    "    def exploreData(self, data_path):\n",
    "        # Read and display the file as pandas dataFrame.\n",
    "        df = pd.read_csv(data_path, delimiter='\\t')\n",
    "        print(df.head(5))\n",
    "\n",
    "        # Get the number of data in the data.\n",
    "        print(f\"The number of data points in train_tsv file are {df.shape[0]}\")\n",
    "\n",
    "        # Check for the number of questions.\n",
    "        unique_questions = df['QuestionID'].unique()\n",
    "        print(f\"The number of unique questions are  {len(unique_questions)}\")\n",
    "\n",
    "        # check the count of no.of documents.\n",
    "        no_of_documents = df['DocumentID'].unique()\n",
    "        print(\n",
    "            f\"The number of documents in train_tsv file is {len(no_of_documents)}\")\n",
    "\n",
    "        # find the average no.of questions per document.\n",
    "        avg_questions_per_doc = df.groupby(\n",
    "            'DocumentID')['QuestionID'].nunique().mean()\n",
    "        # Print the result\n",
    "        print(\"Average number of unique QuestionID values per DocumentID:\",\n",
    "              avg_questions_per_doc)\n",
    "\n",
    "        # get list of documents with more than one question.\n",
    "        docs_with_multiple_questions = df.groupby('DocumentID').filter(\n",
    "            lambda x: x['QuestionID'].nunique() > 1)['DocumentID'].unique()\n",
    "        print(docs_with_multiple_questions)\n",
    "        print(\n",
    "            f\"No of documents with more than one question is {len(docs_with_multiple_questions)} which is {len(no_of_documents) / len(docs_with_multiple_questions)}%\")\n",
    "\n",
    "        # Find the average number of answers per question.\n",
    "        average_no_of_answers_per_question = df.groupby(\n",
    "            'QuestionID')['SentenceID'].nunique().mean()\n",
    "        print(\n",
    "            f\"Average no.of answers per question {average_no_of_answers_per_question}\")\n",
    "\n",
    "    def exploreTrainTsv(self):\n",
    "        print(f\"\\nExploratory DataAnalysis of train_tsv data\")\n",
    "        self.exploreData(self.train_tsv_path)\n",
    "\n",
    "    def exploreDevTsv(self):\n",
    "        print(f\"\\nExploratory DataAnalysis of dev_tsv data\")\n",
    "        self.exploreData(self.dev_tsv_path)\n",
    "\n",
    "    def exploreTestTsv(self):\n",
    "        print(f\"\\nExploratory DataAnalysis of test_tsv data\")\n",
    "        self.exploreData(self.test_tsv_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e4b634",
   "metadata": {},
   "source": [
    "### Obtained Results are as follows:\n",
    "<pre>\n",
    "Exploratory DataAnalysis of train_tsv data\n",
    "\n",
    "  QuestionID                       Question DocumentID DocumentTitle SentenceID                                           Sentence  Label\n",
    "0         Q1  how are glacier caves formed?         D1  Glacier cave       D1-0  A partly submerged glacier cave on Perito More...      0\n",
    "1         Q1  how are glacier caves formed?         D1  Glacier cave       D1-1          The ice facade is approximately 60 m high      0\n",
    "2         Q1  how are glacier caves formed?         D1  Glacier cave       D1-2          Ice formations in the Titlis glacier cave      0\n",
    "3         Q1  how are glacier caves formed?         D1  Glacier cave       D1-3  A glacier cave is a cave formed within the ice...      1\n",
    "4         Q1  how are glacier caves formed?         D1  Glacier cave       D1-4  Glacier caves are often called ice caves , but...      0\n",
    "\n",
    "The number of data points in train_tsv file are 20347\n",
    "The number of unique questions are  2117\n",
    "The number of documents in train_tsv file is 1994\n",
    "\n",
    "Average number of unique QuestionID values per DocumentID: 1.0616850551654964\n",
    "['D6' 'D27' 'D31' 'D41' 'D50' 'D89' 'D100' 'D124' 'D125' 'D128' 'D129'\n",
    " 'D134' 'D155' 'D160' 'D169' 'D179' 'D195' 'D211' 'D212' 'D215' 'D219'\n",
    " 'D230' 'D251' 'D252' 'D255' 'D269' 'D275' 'D292' 'D309' 'D320' 'D342'\n",
    " 'D351' 'D352' 'D359' 'D363' 'D368' 'D370' 'D371' 'D377' 'D380' 'D432'\n",
    " 'D438' 'D449' 'D473' 'D521' 'D528' 'D543' 'D549' 'D557' 'D587' 'D589'\n",
    " 'D607' 'D624' 'D698' 'D729' 'D763' 'D771' 'D773' 'D806' 'D855' 'D857'\n",
    " 'D216' 'D881' 'D885' 'D484' 'D979' 'D1016' 'D1018' 'D1054' 'D1055'\n",
    " 'D1065' 'D1081' 'D1111' 'D1142' 'D1148' 'D1158' 'D1196' 'D1197' 'D1198'\n",
    " 'D1207' 'D1263' 'D1282' 'D1298' 'D1302' 'D1308' 'D1318' 'D1454' 'D1455'\n",
    " 'D1525' 'D1566' 'D1616' 'D1621' 'D1652' 'D660' 'D1703' 'D1832' 'D1855'\n",
    " 'D1868' 'D1876' 'D1559' 'D332' 'D2045' 'D2065' 'D2089' 'D2141' 'D2174'\n",
    " 'D2212' 'D2319' 'D2452' 'D2477']\n",
    " \n",
    "No of documents with more than one question is 110 which is 18.12727272727273%\n",
    "Average no.of answers per question 9.611242324043458\n",
    "\n",
    "Exploratory DataAnalysis of dev_tsv data\n",
    "\n",
    "  QuestionID                                     Question DocumentID     DocumentTitle SentenceID                                           Sentence  Label\n",
    "0         Q8  How are epithelial tissues joined together?         D8  Tissue (biology)       D8-0  Cross section of sclerenchyma fibers in plant ...      0\n",
    "1         Q8  How are epithelial tissues joined together?         D8  Tissue (biology)       D8-1  Microscopic view of a histologic specimen of h...      0\n",
    "2         Q8  How are epithelial tissues joined together?         D8  Tissue (biology)       D8-2  In Biology , Tissue is a cellular organization...      0\n",
    "3         Q8  How are epithelial tissues joined together?         D8  Tissue (biology)       D8-3  A tissue is an ensemble of similar cells from ...      0\n",
    "4         Q8  How are epithelial tissues joined together?         D8  Tissue (biology)       D8-4  Organs are then formed by the functional group...      0\n",
    "\n",
    "The number of data points in train_tsv file are 2733\n",
    "The number of unique questions are  296\n",
    "The number of documents in train_tsv file is 293\n",
    "Average number of unique QuestionID values per DocumentID: 1.0102389078498293\n",
    "['D318' 'D1008' 'D1361']\n",
    "No of documents with more than one question is 3 which is 97.66666666666667%\n",
    "Average no.of answers per question 9.233108108108109\n",
    "\n",
    "Exploratory DataAnalysis of test_tsv data\n",
    "\n",
    "  QuestionID                                         Question DocumentID  ... SentenceID                                           Sentence Label\n",
    "0         Q0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US         D0  ...       D0-0  African immigration to the United States refer...     0\n",
    "1         Q0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US         D0  ...       D0-1  The term African in the scope of this article ...     0\n",
    "2         Q0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US         D0  ...       D0-2  From the Immigration and Nationality Act of 19...     0\n",
    "3         Q0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US         D0  ...       D0-3  African immigrants in the United States come f...     0\n",
    "4         Q0  HOW AFRICAN AMERICANS WERE IMMIGRATED TO THE US         D0  ...       D0-4  They include people from different national, l...     0\n",
    "\n",
    "[5 rows x 7 columns]\n",
    "\n",
    "The number of data points in train_tsv file are 6116\n",
    "The number of unique questions are  630\n",
    "The number of documents in train_tsv file is 616\n",
    "Average number of unique QuestionID values per DocumentID: 1.0227272727272727\n",
    "['D186' 'D270' 'D330' 'D332' 'D660' 'D878' 'D996' 'D1035' 'D1241' 'D1349'\n",
    " 'D1764' 'D2097']\n",
    "No of documents with more than one question is 12 which is 51.333333333333336%\n",
    "Average no.of answers per question 9.707936507936507\n",
    "(tensorflow-gpu) jyothivishnuvardhankolla@Jyothis-MacBook-Pro Chatbot-development % \n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa0597c",
   "metadata": {},
   "source": [
    "# **Step-2:** Data cleaning and preparation. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20d17f2",
   "metadata": {},
   "source": [
    "1. **Remove all the data points which have a sentence which do not contain correct answer.**\n",
    "2. **Get all unique questions in each of the data-split.**\n",
    "3. **Merge all the data-splits with all annotated answers from annotated dataset to create a final dataset with all the valid datapoints.**\n",
    "4. **Create the final valid dataset suitable for training by creating a dataframe with columns Question, Sentence, Answer**\n",
    "\n",
    "#### Key observations:\n",
    "* **There are a total of 1039 questions in the train_data with a proper answer.**\n",
    "* **There are a total of 140 questions in the train_data with a proper answer.**\n",
    "* **There are a total of 291 questions in the train_data with a proper answer.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcb7a1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "if prepare_data == 1:  # If given 1 by user prepares the data need to train the model.\n",
    "        pos_ans_path = \"/Users/jyothivishnuvardhankolla/Desktop/SoftinWay/WikiQACorpus/WikiQASent.pos.ans.tsv\"\n",
    "        # Initialize the PrepareData object.\n",
    "        ob = PrepareData(train_tsv=train_tsv_path, dev_tsv=dev_tsv_path,\n",
    "                         test_tsv=test_tsv_path, pos_ans_tsv=pos_ans_path)\n",
    "        # Preprocess the data and prepares the final data that is ready for training.\n",
    "        ob.Preprocess()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84c0376b",
   "metadata": {},
   "source": [
    "**As shown above it takes the path to annotated datasets and Initializes the object of PrepareData class from Data_prep.py file, then calls Preprocess function which performs all the cleaning and transformations required for fine-tuning the model as shown below**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab304ca",
   "metadata": {},
   "source": [
    "# Data_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eedbbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class PrepareData:\n",
    "    def __init__(self, train_tsv: str, dev_tsv: str, test_tsv: str, pos_ans_tsv: str):\n",
    "        \"\"\"\n",
    "        Creates the Necessary datframes required to create the final_data,\n",
    "        Initialize the object of this class with following args and then call \n",
    "        preprocess function to prepare your data.\n",
    "\n",
    "        ARGS:\n",
    "            train_tsv: Path to train_tsv file.\n",
    "            dev_tsv: Path to dev_tsv file.\n",
    "            test_tsv: Path to test_tsv file.\n",
    "        \"\"\"\n",
    "        self.train_tsv_path = train_tsv\n",
    "        self.dev_tsv_path = dev_tsv\n",
    "        self.test_tsv_path = test_tsv\n",
    "        self.pos_ans_tsv = pos_ans_tsv\n",
    "\n",
    "        # Initialize variables to store dataframes.\n",
    "        self.pos_ans_tsv_df = None\n",
    "        self.train_tsv_df = None\n",
    "        self.dev_tsv_df = None\n",
    "        self.test_tsv_df = None\n",
    "\n",
    "        # Initialize variables to store final train, dev, test dataframes.\n",
    "        self.final_train_df = None\n",
    "        self.final_dev_df = None\n",
    "        self.final_test_df = None\n",
    "\n",
    "    def create_data_frames(self):\n",
    "        # Creates the train, dev, test, pos_ans dataframes.\n",
    "        self.pos_ans_tsv_df = pd.read_csv(self.pos_ans_tsv, delimiter=\"\\t\")\n",
    "        self.train_tsv_df = pd.read_csv(self.train_tsv_path, delimiter=\"\\t\")\n",
    "        self.dev_tsv_df = pd.read_csv(self.dev_tsv_path, delimiter=\"\\t\")\n",
    "        self.test_tsv_df = pd.read_csv(self.test_tsv_path, delimiter=\"\\t\")\n",
    "\n",
    "    def create_final_df(self, df: pd.DataFrame):\n",
    "        # Takes in a dataframe and creates a new df ready for training\n",
    "        final_dict = {\n",
    "            'question': [],\n",
    "            'sentence': [],\n",
    "            'answer': []\n",
    "        }\n",
    "\n",
    "        for i, row in df.iterrows():\n",
    "            # Iterate through each row and create new row for each of correct answer.\n",
    "            if pd.notna(row['AnswerPhrase1']) and row['AnswerPhrase1'] != 'NO_ANS':\n",
    "                final_dict['question'].append(row['Question'])\n",
    "                final_dict['sentence'].append(row['Sentence'])\n",
    "                final_dict['answer'].append(row['AnswerPhrase1'])\n",
    "\n",
    "            if pd.notna(row['AnswerPhrase2']) and row['AnswerPhrase2'] != 'NO_ANS':\n",
    "                final_dict['question'].append(row['Question'])\n",
    "                final_dict['sentence'].append(row['Sentence'])\n",
    "                final_dict['answer'].append(row['AnswerPhrase2'])\n",
    "\n",
    "            if pd.notna(row['AnswerPhrase3']) and row['AnswerPhrase3'] != 'NO_ANS':\n",
    "                final_dict['question'].append(row['Question'])\n",
    "                final_dict['sentence'].append(row['Sentence'])\n",
    "                final_dict['answer'].append(row['AnswerPhrase3'])\n",
    "\n",
    "        return pd.DataFrame(final_dict)\n",
    "\n",
    "    def Preprocess(self):\n",
    "        # Creates the final cleaned datasets and stored them in disk.\n",
    "        self.create_data_frames()  # Create dataframes.\n",
    "        # Preprocess the dataframes(eliminate all rows with incorrect sentences.)\n",
    "        cleaned_train_tsv_data = self.train_tsv_df[self.train_tsv_df[\"Label\"] == 1]\n",
    "        cleaned_dev_tsv_data = self.dev_tsv_df[self.dev_tsv_df[\"Label\"] == 1]\n",
    "        cleaned_test_tsv_data = self.test_tsv_df[self.test_tsv_df[\"Label\"] == 1]\n",
    "\n",
    "        # Get list of all unique_questions in the cleaned data.\n",
    "        unique_questions_train = cleaned_train_tsv_data['QuestionID'].unique()\n",
    "        unique_questions_dev = cleaned_dev_tsv_data['QuestionID'].unique()\n",
    "        unique_questions_test = cleaned_test_tsv_data['QuestionID'].unique()\n",
    "\n",
    "        # separate train_pos_ans_df for each of train, dev, test.\n",
    "        train_pos_ans_tsv = self.pos_ans_tsv_df[self.pos_ans_tsv_df['QuestionID'].isin(\n",
    "            unique_questions_train)]\n",
    "        dev_pos_ans_tsv = self.pos_ans_tsv_df[self.pos_ans_tsv_df['QuestionID'].isin(\n",
    "            unique_questions_dev)]\n",
    "        test_pos_ans_tsv = self.pos_ans_tsv_df[self.pos_ans_tsv_df['QuestionID'].isin(\n",
    "            unique_questions_test)]\n",
    "\n",
    "        # Merge train, dev, test dataframes with pos_ans dataframes.\n",
    "        merged_train_df = pd.merge(train_pos_ans_tsv, cleaned_train_tsv_data, on=[\n",
    "                                   \"QuestionID\", \"Question\", \"DocumentID\", \"DocumentTitle\", \"SentenceID\", \"Sentence\"])\n",
    "        merged_dev_df = pd.merge(dev_pos_ans_tsv, cleaned_dev_tsv_data, on=[\n",
    "                                 \"QuestionID\", \"Question\", \"DocumentID\", \"DocumentTitle\", \"SentenceID\", \"Sentence\"])\n",
    "        merged_test_df = pd.merge(test_pos_ans_tsv, cleaned_test_tsv_data, on=[\n",
    "                                  \"QuestionID\", \"Question\", \"DocumentID\", \"DocumentTitle\", \"SentenceID\", \"Sentence\"])\n",
    "\n",
    "        # Create the final dataframes.\n",
    "        self.final_train_df = self.create_final_df(merged_train_df)\n",
    "        self.final_dev_df = self.create_final_df(merged_dev_df)\n",
    "        self.final_test_df = self.create_final_df(merged_test_df)\n",
    "\n",
    "        # Save the final dataframes in a csv file.\n",
    "        self.final_train_df.to_csv(\"data/train.csv\", index=False)\n",
    "        self.final_dev_df.to_csv(\"data/dev.csv\", index=False)\n",
    "        self.final_test_df.to_csv(\"data/test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aaf1e97",
   "metadata": {},
   "source": [
    "**The final data-set contains a total of 1941 data points with Question,Sentence,Answer columns**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ecd5c",
   "metadata": {},
   "source": [
    "# Step-3:Model training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c440a3e6",
   "metadata": {},
   "source": [
    "**I have used Fine-tuned Bert to build the question-answering system.**\n",
    "* First lets discuss about the Bert model.\n",
    "<img></img>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
